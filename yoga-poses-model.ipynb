{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":1559111,"sourceType":"datasetVersion","datasetId":920599}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport os\nimport random\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4mLLpXPQLaH7","outputId":"5c00cf03-5d44-4c70-f101-4a307cb0e90c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_data = []\ntest_data = []\n\nfor root, _, files in os.walk('/kaggle/input/yoga-poses-dataset'):\n    for file in files:\n        if file.endswith('.jpg'):\n            image_path = os.path.join(root, file)\n            label = os.path.basename(root)  # Assuming folder name is the label\n\n            # Adjust this part based on where you want to add the data\n            if 'TRAIN' in root:  # Check if 'TRAIN' is in the folder path\n                train_data.append([image_path, label])\n            elif 'TEST' in root:  # Check if 'TEST' is in the folder path\n                test_data.append([image_path, label])\n\n\ntrain_df = pd.DataFrame(train_data, columns=['image', 'label'])\ntest_df = pd.DataFrame(test_data, columns=['image', 'label'])\n\nprint(test_df.head())\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2Uh32JnLchp","outputId":"213bd845-a85c-4fa3-8479-99ecc99978d8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom PIL import Image\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nclass MyDataset(Dataset):\n    def __init__(self, df, image_column, label_column, transform=None):\n        self.df = df\n        self.image_column = image_column\n        self.label_column = label_column\n        self.transform = transform\n\n        # Create and fit a LabelEncoder for labels\n        self.label_encoder = LabelEncoder()\n        self.label_encoder.fit(self.df[self.label_column])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_path = self.df.iloc[idx][self.image_column]\n\n        # Load the image using PIL (Pillow)\n        image = Image.open(image_path).convert('RGB')\n\n        label = self.df.iloc[idx][self.label_column]\n\n        # Transform label to numerical using LabelEncoder\n        label = self.label_encoder.transform([label])[0]\n\n        # Apply transformations only if they are provided\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label","metadata":{"id":"J-XV7EvQLd9w","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the target size for your images\ntarget_size = (224, 224)  # Example: Resize to 224x224\n\n# Modified transformations with resizing\ntrain_transform = transforms.Compose([\n    transforms.Resize(target_size),  # Resize the image\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize(target_size),  # Resize the image\n    transforms.ToTensor(),\n])","metadata":{"id":"OQJXtqq4LfiB","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_images(df, image_column, transform):\n    \"\"\"Resizes and transforms images in a DataFrame.\"\"\"\n    for index in df.index:\n        image_path = df.loc[index, image_column]\n        try:\n            # Load the image\n            image = Image.open(image_path).convert('RGB')\n\n            # Print image size before transformation (for debugging)\n            print(f\"Original image size: {image.size}\")\n\n            # Apply the transformation\n            transformed_image = transform(image)\n\n            # Print image size after transformation (for debugging)\n            print(f\"Transformed image size: {transformed_image.shape}\")\n\n            # Save the transformed image back to the same path\n            os.remove(image_path)  # Remove original image before saving\n            transformed_image_pil = transforms.ToPILImage()(transformed_image)\n            transformed_image_pil.save(image_path)\n\n        except Exception as e:\n            print(f\"Error processing image {image_path}: {e}\")\n            # Handle the error (e.g., skip the image, remove it from the DataFrame)\n","metadata":{"id":"oAJlfuZDLhBg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_collate(batch):\n    \"\"\"Pads images in a batch to the maximum size.\"\"\"\n    # Get maximum image dimensions\n    max_width = max(image.shape[2] for image, label in batch)\n    max_height = max(image.shape[1] for image, label in batch)\n\n    # Pad images\n    padded_images = []\n    labels = []\n    for image, label in batch:\n        pad_width = max_width - image.shape[2]\n        pad_height = max_height - image.shape[1]\n        padded_image = torch.nn.functional.pad(image, (0, pad_width, 0, pad_height))\n        padded_images.append(padded_image)\n        labels.append(label)\n\n    # Stack padded images and labels\n    return torch.stack(padded_images), torch.tensor(labels)\n\nbatch_size = 16","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = MyDataset(train_df, image_column='image', label_column='label', transform=train_transform)\ntest_dataset = MyDataset(test_df, image_column='image', label_column='label', transform=test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size,collate_fn=custom_collate, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=custom_collate, shuffle=False)","metadata":{"id":"tM6eAuhlLjB4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"gbAJ2rWqLkUY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the model (similar to the TensorFlow model)\nclass CNNModel(nn.Module):\n    def __init__(self, num_classes):\n        super(CNNModel, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout(0.25),\n            nn.Flatten(),\n            nn.Linear(256 * 14 * 14, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(1024, num_classes)\n        )\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"id":"nwoU_ntDLluo","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate the model, loss function, and optimizer\nnum_classes = len(train_df['label'].unique())\nmodel = CNNModel(num_classes=num_classes)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"id":"h-Wd1wykLnG4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### Training loop\nepochs = 50\ntrain_losses, train_accuracies = [], []\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_accuracy = 100 * correct / total\n    train_losses.append(epoch_loss)\n    train_accuracies.append(epoch_accuracy)\n    print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QNcDX_LxLoWw","outputId":"245b04b4-d2ac-4156-b9e8-ac59165939de","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Plotting training loss and accuracy\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(train_accuracies, label='Train Accuracy')\nplt.xlabel('Epoch')\n\n# Plotting training loss and accuracy\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(train_accuracies, label='Train Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(train_losses, label='Train Loss')\n\n# Produits payants Colab - Résilier les contrats ici\n# En attente de finalisation de l'exécution actuelle.\n\nplt.ylabel('Accuracy')\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(train_losses, label='Train Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()# l'exécution actuelle\n\n# Evaluation on test data\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Test Accuracy: {100 * correct / total:.2f}%')","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"id":"ZHILvEMsLqM4","outputId":"6ae7e67f-2f2a-4b41-c13e-0b146651a547","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Model = CNNModel(num_classes=10)\ntorch.save(Model.state_dict(),'full_Model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}